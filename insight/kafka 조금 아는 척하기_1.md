### ✅ 카프카란?

**분산 이벤트 스트리밍 플랫폼**

- 이벤트 스트리밍을 처리하기 위한
- 고성능 플랫폼
- 많은 기업들이 사용 중

### ✅ 기본 구조: 4개의 구성요소

![image.png](../.images/kafka_simple_1.png)

**카프카 클러스터**

- 메시지(이벤트) 저장소
- 여러 개의 브로커로 구성됨 → 브로커를 각각의 서버라고 보면 됨
- 브로커들이 메시지를 나눠서 저장, 이중화 처리, 장애 대체 등의 역할을 함

**주키퍼 클러스터(앙상블)**

- 카프카 클러스터 관리
- 카프카 클러스터에 대한 정보 기록 및 관리

**프로듀서(Producer)**

- 카프카 클러스터에 메시지를 보내는 것
- 메시지(이벤트)를 카프카에 넣는 역할

**컨슈머(consumer)**

- 메시지(이벤트)를 카프카에서 읽어오는 역할

즉 프로듀서가 카프카 클러스터에 메시지를 넣고, 컨슈머는 그 메시지를 읽어와서 필요한 처리를 한다.

따라서 카프카 클러스터는 데이터를 이동하는데 필요한 핵심 역할을 맡게 된다.

### ✅ 토픽과 파티션

![image.png](../.images/kafka_simple_2.png)

**토픽: 메시지를 구분하는 단위**

- 뉴스용 토픽, 주문용 토픽 등
- 각각의 메시지를 알맞게 구분하기 위한 용도
- 파일시스템의 폴더와 유사
- 한 개의 토픽은 한 개 이상의 파티션으로 구성

> 프로듀서는 메시지를 카프카에 저장할 때, 어떤 토픽에 저장해줘라고 요청 -> 
> 컨슈머는 어떤 토픽에서 메시지를 읽어올래라고 함
> 
> 이렇게 프로듀서와 컨슈머가 토픽을 기준으로 메시지를 주고받게 됨

**파티션: 메시지를 저장하는 물리적인 파일**

![image.png](../.images/kafka_simple_3.png)

- 추가만 가능한 append-only 파일 → 카프카가 일부를 삭제, 출력하긴 하지만 기본적으로
- 각 메시지 저장 위치를 **오프셋(offset)**이라고 함
- 프로듀서가 넣은 메시지는 파티션의 맨 뒤에 추가
- 컨슈머는 오프셋 기준으로 메시지를 **순서대로** 읽음
- 메시지는 삭제되지 않음(설정에 따라 일정 시간이 지난 뒤 삭제)

### ✅ 여러 파티션과 프로듀서

**토픽이 여러 파티션으로 구성될 수 있다고 했음 → 그럼 프로듀서는 어떤 파티션에 메시지를 저장?**

![image.png](../.images/kafka_simple_4.png)

- 라운드로빈으로 돌아가면서 저장을 하거나 키를 이용해 파티션 선택
- 프로듀서가 카프카에 메시지를 전송할 때, 토픽의 이름뿐만아니라 키도 지정 가능

  → 같은 해시값을 이용해서 저장할 토픽 선택

- 즉, 같은 키를 갖는 메시지는  같은 파티션에 저장 → 같은 키는 순서 유지

### ✅ 여러 파티션과 컨슈머

![image.png](../.images/kafka_simple_5.png)

- 컨슈머는 컨슈머그룹에 속함
- 한 개 파티션은 컨슈머그룹의 한 개 컨슈머만 연결 가능
- 즉, 컨슈머그룹에 속한 컨슈머들은 한 파티션을 공유할 수 없음

  → 한 컨슈머그룹 기준으로 파티션의 메시지는 순서대로 처리 가능


### ✅ 성능

**파티션 파일은 OS 페이지캐시 사용**

- 파티션에 대한 파일 IO를 메모리에서 처리
- 서버에서 페이지캐시를 카프카만 사용해서 성능에 유리

**Zero Copy**

- 디스크 버퍼에서 네트워크 버퍼로 직접 데이터 복사 → 속도가 빨라짐

**컨슈머 추적을 위해 브로커가 하는 일이 비교적 단순**

- 메시지 필터, 메시지 재전송과 같은 일은 브로커가 하지 않음 → 프로듀서, 컨슈머가 직접 해야함
- 브로커는 컨슈머와 파티션 간 매핑 관리

**묶어서 보내기, 묶어서 받기 (batch)**

![image.png](../.images/kafka_simple_6.png)

- 프로듀서: 일정 크기만큼 메시지를 모아서 전송 가능
- 컨슈머: 최소 크기(일정 크기 이상)만큼 메시지를 모아서 조회 가능

→ 낱개 처리보다 처리량 증가

**처리량 증대(확장)가 쉬움**

![image.png](../.images/kafka_simple_7.png)

- 1개 장비의 용량 한계 → 브로커 추가, 파티션 추가
- 컨슈머가 느림 → 컨슈머 추가 (+파티션 추가)
- 수평 확장이 용이한 구조

### ✅ 리플리카 - 복제

카프카는 장애가 낫을 때 대처하기 위해 리플리카라는 것을 사용함

![image.png](../.images/kafka_simple_8.png)

**리플리카: 파티션의 복제본**

- 복제수(replication factor) 만큼 파티션의 복제본이 각 브로커에 생김

**리더와 팔로워로 구성**

- 여러개의 리플리카 중 하나가 리더가 되고 나머지는 팔로워가됨
- 프로듀서와 컨슈머는 리더를 통해서만 메시지 처리
- 팔로워는 리더로부터 복제(데이터를 읽어와서 저장)

**장애 대응**

- 리더가 속한 브로커 장애 시 다른 팔로워가 리더가 됨

### ✅ 정리

**성능(높은 처리량)**

- OS 페이지캐시
- (다른 메시징 시스템 대비) 단순한 브로커
- 묶어서 데이터 전송
- 파티션/컨슈머 추가로 수평 확장

**고가용성**

- 파티션 리플리케이션 + 리더/팔로워 구조

### ✅ 찾아 보기

- Zero copy
- OS 페이지캐시

### ✅ 궁금한 점

Q. 만약 카프카 클러스터를 관리하는 주키퍼 클러스터가 불안정하거나 죽는다면 카프카도 완전히 사용 불가한걸까? 이미 연결되어있던 컨슈머와 프로듀서도 중단되어 장애가 발생할까?
(주키퍼도 클러스터 형태이지 이런 케이스가 거의 없겠지만?)

Q. 카프카 장애 발생 시 리플리카를 통해 리더 대신 다른 팔로워가 리더가 된다고 했는데 이 리더가 바뀌는 동안에 들어오는 메시지에 대해 손실 가능성은 없을까?

[🔗 출처 링크](https://www.youtube.com/watch?v=0Ssx7jJJADI&t=33s)
